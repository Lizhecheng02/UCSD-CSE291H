{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import defaultdict\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_segment(text):\n",
    "    return text.strip().replace(\" \", \"_\") if text.strip() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_segmented_corpus(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    processed_sentences = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        tokens = []\n",
    "        current_pos = 0\n",
    "        line_length = len(line)\n",
    "\n",
    "        while current_pos < line_length:\n",
    "            next_phrase_start = line.find(\"<phrase_Q=\", current_pos)\n",
    "\n",
    "            if next_phrase_start == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            if next_phrase_start > current_pos:\n",
    "                pre_phrase_text = line[current_pos:next_phrase_start]\n",
    "                pre_phrase_token = process_text_segment(pre_phrase_text)\n",
    "                if pre_phrase_token:\n",
    "                    tokens.append(pre_phrase_token)\n",
    "\n",
    "            phrase_header_end = line.find(\">\", next_phrase_start)\n",
    "            if phrase_header_end == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            phrase_end = line.find(\"</phrase>\", phrase_header_end)\n",
    "            if phrase_end == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            phrase = line[phrase_header_end + 1:phrase_end]\n",
    "            if phrase:\n",
    "                phrase_token = phrase.replace(\" \", \"_\")\n",
    "                tokens.append(phrase_token)\n",
    "\n",
    "            current_pos = phrase_end + len(\"</phrase>\")\n",
    "\n",
    "        if tokens:\n",
    "            processed_sentences.append(tokens)\n",
    "\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentences, vector_size=100, window=5, min_count=1):\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 5000\n",
      "Sentence 1: ['OQL', '[', 'C++', ']:', 'Extending', 'C++', 'with_an_Object_Query_Capability.']\n",
      "Sentence 2: ['Transaction_Management', 'in', 'Multidatabase_Systems', '.']\n",
      "Sentence 3: ['Overview', 'of_the_ADDS_System.']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./AutoPhrase/models/DBLP/segmentation.txt\"\n",
    "\n",
    "sentences = parse_segmented_corpus(file_path)\n",
    "print(\"Total number of sentences:\", len(sentences))\n",
    "\n",
    "for i, sentence in enumerate(sentences[:3]):\n",
    "    print(f\"Sentence {i + 1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 23525\n",
      "Example vectors:\n",
      "\n",
      "Token: OQL\n",
      "Vector shape: (100,)\n",
      "First 5 dimensions: [-0.00329331  0.00341695 -0.00770479  0.00470063  0.00434671]\n",
      "\n",
      "Token: [\n",
      "Vector shape: (100,)\n",
      "First 5 dimensions: [ 0.0034336  -0.0016726   0.00129306 -0.00726168  0.00917324]\n"
     ]
    }
   ],
   "source": [
    "model = train_word2vec(sentences)\n",
    "print(f\"Total vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "\n",
    "print(\"Example vectors:\")\n",
    "for sentence in sentences[:1]:\n",
    "    for token in sentence[:2]:\n",
    "        print(f\"\\nToken: {token}\")\n",
    "        print(f\"Vector shape: {model.wv[token].shape}\")\n",
    "        print(f\"First 5 dimensions: {model.wv[token][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_kmeans(model, num_clusters=5):    \n",
    "    vectors = []\n",
    "    words = []\n",
    "    for word in model.wv.index_to_key:\n",
    "        vectors.append(model.wv[word])\n",
    "        words.append(word)\n",
    "    \n",
    "    X = np.array(vectors)\n",
    "    print(X.shape)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    cluster_dict = defaultdict(list)\n",
    "    for word, cluster in zip(words, clusters):\n",
    "        cluster_dict[cluster].append(word)\n",
    "    \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_gmm(model, num_clusters=6):\n",
    "    vectors = []\n",
    "    words = []\n",
    "    for word in model.wv.index_to_key:\n",
    "        vectors.append(model.wv[word])\n",
    "        words.append(word)\n",
    "    \n",
    "    X = np.array(vectors)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=num_clusters, random_state=42)\n",
    "    clusters = gmm.fit_predict(X)\n",
    "\n",
    "    cluster_dict = defaultdict(list)\n",
    "    for word, cluster in zip(words, clusters):\n",
    "        cluster_dict[cluster].append(word)\n",
    "    \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_samples(cluster_dict, num_samples=20):\n",
    "    print(\"Cluster Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for cluster_id in sorted(cluster_dict.keys()):\n",
    "        words = cluster_dict[cluster_id]\n",
    "        samples = words if len(words) <= num_samples else random.sample(words, num_samples)\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id + 1} (Total words: {len(words)}):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, word in enumerate(samples, 1):\n",
    "            print(f\"{i}. {word.replace('_', ' ')}\", end=\"\\n\" if (i + 1) % 5 == 0 else \" | \")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23525, 100)\n",
      "Cluster Analysis Results:\n",
      "==================================================\n",
      "\n",
      "Cluster 1 (Total words: 6573):\n",
      "--------------------------------------------------\n",
      "1. point bases, these images are | 2. and Development of | 3. development is known as | 4. Networks of Workstations\n",
      "5. image of the object recorded by a | 6. radially symmetric | 7. for all recorded images. These | 8. as the problem of | 9. By\n",
      "10. trust | 11. which are directly used, or are the basis, for | 12. where he studied as a | 13. Scene Analysis | 14. Systems of Reductions\n",
      "15. Scalable | 16. Resolution Theorem Proving | 17. Jasmine | 18. fashion | 19. approaches the desired\n",
      "20. that applies for any isovalue. As | \n",
      "\n",
      "\n",
      "Cluster 2 (Total words: 6459):\n",
      "--------------------------------------------------\n",
      "1. The TSQL2 | 2. Legislation | 3. unambiguously and dynamically. The | 4. Critical Realism\n",
      "5. , Sequencing and Transformations. | 6. . Lippman dispells the misinformation and | 7. , the system achieves a | 8. Ethical | 9. specific parameters\n",
      "10. Ensuring | 11. proportional | 12. modeler | 13. page for this | 14. physical database\n",
      "15. an Interconnected | 16. plans in some domains where no strong or strong | 17. -Occlusions. | 18. Sharpness. | 19. Patterns\n",
      "20. - An Empirical Approach | \n",
      "\n",
      "\n",
      "Cluster 3 (Total words: 3636):\n",
      "--------------------------------------------------\n",
      "1. displacement maps | 2. planning domain | 3. Matchmaking | 4. an\n",
      "5. Ada | 6. Looking around in our every day environment, many of the encountered objects are | 7. . Further, it argues that implementation of systems is | 8. , but these have always implied an important | 9. robot\n",
      "10. nature | 11. using a | 12. Using a | 13. simulation | 14. requires a base surface, this\n",
      "15. photo | 16. , we propose | 17. . Each tool needed to have a | 18. Documents. | 19. -Row Telemeter and\n",
      "20. Region | \n",
      "\n",
      "\n",
      "Cluster 4 (Total words: 6707):\n",
      "--------------------------------------------------\n",
      "1. Mechanismen | 2. Visual Hulls | 3. Object and | 4. Entwurfs- und\n",
      "5. enriched with | 6. It is well known that many surfaces exhibit | 7. enhancing | 8. measures, we | 9. Algorithmischen\n",
      "10. Networks | 11. , the TLATEX | 12. techniques which use | 13. die | 14. to produce\n",
      "15. Motion Analysis | 16. continuation-passing style | 17. : Beyond | 18. image, | 19. Branching\n",
      "20. -4 | \n",
      "\n",
      "\n",
      "Cluster 5 (Total words: 23):\n",
      "--------------------------------------------------\n",
      "1. the | 2. . The | 3. of | 4. -\n",
      "5. : | 6. database | 7. to | 8. . | 9. and\n",
      "10. results | 11. ( | 12. model | 13. of the | 14. in\n",
      "15. range | 16. information | 17. data | 18. , | 19. paper\n",
      "20. D | \n",
      "\n",
      "\n",
      "Cluster 6 (Total words: 127):\n",
      "--------------------------------------------------\n",
      "1. Algorithms | 2. area | 3. Results | 4. major\n",
      "5. SQL | 6. color | 7. is | 8. Tcl | 9. state\n",
      "10. databases | 11. Web | 12. light | 13. solution | 14. Modeling\n",
      "15. partial | 16. extended | 17. acquisition | 18. first | 19. understanding\n",
      "20. ) | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 6\n",
    "cluster_dict = perform_clustering_kmeans(model, num_clusters)\n",
    "print_cluster_samples(cluster_dict, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Analysis Results:\n",
      "==================================================\n",
      "\n",
      "Cluster 1 (Total words: 6671):\n",
      "--------------------------------------------------\n",
      "1. is measured and | 2. Rigid Motion | 3. tilings | 4. , compared with\n",
      "5. Colon | 6. Users Access to | 7. problem should be solved between the appearance and | 8. Objects with | 9. , two\n",
      "10. x-y | 11. a conclusion and an | 12. the ýidealý generatrix derived from the | 13. collected over approximately 8 | 14. and intensity\n",
      "15. closes with a list of some remaining | 16. -Shaped | 17. Situationsmodellierung in der Bildfolgeauswertung | 18. combination image\", which relies on those edges of the | 19. Interactivity\n",
      "20. \" in the | \n",
      "\n",
      "\n",
      "Cluster 2 (Total words: 6823):\n",
      "--------------------------------------------------\n",
      "1. relationale Datenbanken | 2. images measured with | 3. Method: Use of Motion in | 4. Preparedness\n",
      "5. Spatiotemporal | 6. even after the shape had suffered various modifications. Two | 7. Supporting Tools | 8. Invariant | 9. preserves the boundaries while inner regions are\n",
      "10. third normal form | 11. relighting | 12. Challenges for | 13. amount can be regulated according to access constraints. | 14. 3D Structure and Motions from an\n",
      "15. continuously changing | 16. Electronics | 17. Orthogonality | 18. Question-Answering | 19. images, and the\n",
      "20. that this increased | \n",
      "\n",
      "\n",
      "Cluster 3 (Total words: 3070):\n",
      "--------------------------------------------------\n",
      "1. computational cost | 2. procedure for setting the | 3. considerable attention | 4. American\n",
      "5. Multiparticipant | 6. phase include: a) a complete object | 7. lossy | 8. approximate visibility | 9. Ode\n",
      "10. multiple range images | 11. Examining | 12. Champion | 13. schema | 14. Semantic\n",
      "15. computational problems | 16. Conceptual | 17. brings together | 18. procedural | 19. Training\n",
      "20. and to show how these principles apply to | \n",
      "\n",
      "\n",
      "Cluster 4 (Total words: 6811):\n",
      "--------------------------------------------------\n",
      "1. is included, | 2. ; several issues remain to be resolved to make these tools useful for | 3. Correctness of | 4. /IM.\n",
      "5. Radon Transform | 6. directions in such a way that the different textures are | 7. Instructional Design | 8. dedicated to a particular domain. Our examples concern | 9. Supply Chain\n",
      "10. of queries against | 11. Computational Models | 12. Case-Based Learning | 13. Updating | 14. the Past.\n",
      "15. terms to increase the robustness and to keep the segmentation of the | 16. patch-based | 17. Theory and | 18. at all. | 19. Agent-Based\n",
      "20. efficiently. The | \n",
      "\n",
      "\n",
      "Cluster 5 (Total words: 23):\n",
      "--------------------------------------------------\n",
      "1. . The | 2. of | 3. paper | 4. :\n",
      "5. and | 6. in | 7. - | 8. D | 9. of the\n",
      "10. to | 11. algorithm | 12. range | 13. ( | 14. for\n",
      "15. . | 16. information | 17. data | 18. , | 19. database\n",
      "20. book | \n",
      "\n",
      "\n",
      "Cluster 6 (Total words: 127):\n",
      "--------------------------------------------------\n",
      "1. UML | 2. chapter | 3. real-time | 4. general\n",
      "5. laser | 6. Design | 7. Programming | 8. geometric | 9. registration\n",
      "10. algorithms | 11. by | 12. practical | 13. with | 14. mesh\n",
      "15. a | 16. , and | 17. are | 18. virtual | 19. Model\n",
      "20. Database Systems | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 6\n",
    "cluster_dict = perform_clustering_gmm(model, num_clusters)\n",
    "print_cluster_samples(cluster_dict, num_samples=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
