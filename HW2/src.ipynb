{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import defaultdict\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_segment(text):\n",
    "    return text.strip().replace(\" \", \"_\") if text.strip() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_segmented_corpus(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    processed_sentences = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        tokens = []\n",
    "        current_pos = 0\n",
    "        line_length = len(line)\n",
    "\n",
    "        while current_pos < line_length:\n",
    "            next_phrase_start = line.find(\"<phrase_Q=\", current_pos)\n",
    "\n",
    "            if next_phrase_start == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            if next_phrase_start > current_pos:\n",
    "                pre_phrase_text = line[current_pos:next_phrase_start]\n",
    "                pre_phrase_token = process_text_segment(pre_phrase_text)\n",
    "                if pre_phrase_token:\n",
    "                    tokens.append(pre_phrase_token)\n",
    "\n",
    "            phrase_header_end = line.find(\">\", next_phrase_start)\n",
    "            if phrase_header_end == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            phrase_end = line.find(\"</phrase>\", phrase_header_end)\n",
    "            if phrase_end == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            phrase = line[phrase_header_end + 1:phrase_end]\n",
    "            if phrase:\n",
    "                phrase_token = phrase.replace(\" \", \"_\")\n",
    "                tokens.append(phrase_token)\n",
    "\n",
    "            current_pos = phrase_end + len(\"</phrase>\")\n",
    "\n",
    "        if tokens:\n",
    "            processed_sentences.append(tokens)\n",
    "\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentences, vector_size=100, window=5, min_count=1):\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 5000\n",
      "Sentence 1: ['OQL', '[', 'C++', ']:', 'Extending', 'C++', 'with_an_Object_Query_Capability.']\n",
      "Sentence 2: ['Transaction_Management', 'in', 'Multidatabase_Systems', '.']\n",
      "Sentence 3: ['Overview', 'of_the_ADDS_System.']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./AutoPhrase/models/DBLP/segmentation.txt\"\n",
    "\n",
    "sentences = parse_segmented_corpus(file_path)\n",
    "print(\"Total number of sentences:\", len(sentences))\n",
    "\n",
    "for i, sentence in enumerate(sentences[:3]):\n",
    "    print(f\"Sentence {i + 1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 23525\n",
      "Example vectors:\n",
      "\n",
      "Token: OQL\n",
      "Vector shape: (100,)\n",
      "First 5 dimensions: [-0.00332852  0.00350538 -0.00760019  0.0047511   0.00425787]\n",
      "\n",
      "Token: [\n",
      "Vector shape: (100,)\n",
      "First 5 dimensions: [ 4.4157873e-03 -1.9596594e-03  9.1175600e-05 -8.6908219e-03\n",
      "  8.6680157e-03]\n"
     ]
    }
   ],
   "source": [
    "model = train_word2vec(sentences)\n",
    "print(f\"Total vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "\n",
    "print(\"Example vectors:\")\n",
    "for sentence in sentences[:1]:\n",
    "    for token in sentence[:2]:\n",
    "        print(f\"\\nToken: {token}\")\n",
    "        print(f\"Vector shape: {model.wv[token].shape}\")\n",
    "        print(f\"First 5 dimensions: {model.wv[token][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_kmeans(model, num_clusters=5):    \n",
    "    vectors = []\n",
    "    words = []\n",
    "    for word in model.wv.index_to_key:\n",
    "        vectors.append(model.wv[word])\n",
    "        words.append(word)\n",
    "    \n",
    "    X = np.array(vectors)\n",
    "    print(X.shape)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    cluster_dict = defaultdict(list)\n",
    "    for word, cluster in zip(words, clusters):\n",
    "        cluster_dict[cluster].append(word)\n",
    "    \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_gmm(model, num_clusters=6):\n",
    "    vectors = []\n",
    "    words = []\n",
    "    for word in model.wv.index_to_key:\n",
    "        vectors.append(model.wv[word])\n",
    "        words.append(word)\n",
    "    \n",
    "    X = np.array(vectors)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=num_clusters, random_state=42)\n",
    "    clusters = gmm.fit_predict(X)\n",
    "\n",
    "    cluster_dict = defaultdict(list)\n",
    "    for word, cluster in zip(words, clusters):\n",
    "        cluster_dict[cluster].append(word)\n",
    "    \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_samples(cluster_dict, num_samples=20):\n",
    "    print(\"Cluster Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for cluster_id in sorted(cluster_dict.keys()):\n",
    "        words = cluster_dict[cluster_id]\n",
    "        samples = words if len(words) <= num_samples else random.sample(words, num_samples)\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id + 1} (Total words: {len(words)}):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, word in enumerate(samples, 1):\n",
    "            print(f\"{i}. {word.replace('_', ' ')}\", end=\"\\n\" if i % 5 == 0 else \" | \")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use KMeans to perform clustering and randomly print 20 phrases in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23525, 100)\n",
      "Cluster Analysis Results:\n",
      "==================================================\n",
      "\n",
      "Cluster 1 (Total words: 6498):\n",
      "--------------------------------------------------\n",
      "1. . The method is based on manual | 2. they save space as well as time and also facilitate operations such as search. Examples are given of the use of these | 3. leafs | 4. Scans as a | 5. operators and\n",
      "6. protocols have quietly vanished,, and the | 7. Conventional | 8. and the projector are moved at the same time. | 9. few years have seen a | 10. Novel\n",
      "11. structure of the | 12. among designers and planners, no one may have all of the | 13. actively studied | 14. constraints for which we use a | 15. Attention System for\n",
      "16. (but only to instructors). The file with the figures and the | 17. the Understandability of | 18. RTSP | 19. queries gives not only an | 20. Separierung und\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2 (Total words: 3814):\n",
      "--------------------------------------------------\n",
      "1. to fit the bore | 2. approximation algorithm | 3. partitions | 4. collision avoidance | 5. System Development.\n",
      "6. to the | 7. with current | 8. . He has published widely on | 9. Human Body | 10. is based on\n",
      "11. off, providing a | 12. trees | 13. European | 14. applying | 15. experiments demonstrate\n",
      "16. query tree | 17. images captured | 18. local similarity | 19. . The system includes a | 20. in preventing\n",
      "\n",
      "\n",
      "\n",
      "Cluster 3 (Total words: 123):\n",
      "--------------------------------------------------\n",
      "1. Web | 2. sequence | 3. Efficient | 4. visual | 5. Tk\n",
      "6. Java | 7. research | 8. 3D | 9. Using | 10. general\n",
      "11. sensor | 12. automatic | 13. schema | 14. Prolog | 15. Data\n",
      "16. The | 17. second | 18. Range | 19. a | 20. on\n",
      "\n",
      "\n",
      "\n",
      "Cluster 4 (Total words: 6549):\n",
      "--------------------------------------------------\n",
      "1. . By creating a | 2. . Reflectivity variations recorded at the surface by | 3. representation. Each | 4. from objects. The current approach works on 3D plus brightness or | 5. On the Shape of the\n",
      "6. IT- | 7. took from | 8. height recovery | 9. 68 | 10. natural sciences\n",
      "11. multiperspective | 12. . Current | 13. Rule Induction | 14. Nguyen | 15. leads to a non-\n",
      "16. , we will review several different 3D scanning devices. We will present a method for empirical accuracy analysis, and apply it to several scanners providing | 17. Game | 18. , support for encapsulation, | 19. to measure the accuracy of our method. | 20. Entwerfen großer Programmsysteme:\n",
      "\n",
      "\n",
      "\n",
      "Cluster 5 (Total words: 24):\n",
      "--------------------------------------------------\n",
      "1. to | 2. the | 3. in | 4. - | 5. results\n",
      "6. . The | 7. for | 8. book | 9. model | 10. of the\n",
      "11. database | 12. D | 13. camera | 14. information | 15. of\n",
      "16. data | 17. ( | 18. algorithm | 19. , | 20. range\n",
      "\n",
      "\n",
      "\n",
      "Cluster 6 (Total words: 6517):\n",
      "--------------------------------------------------\n",
      "1. can be recovered for the whole | 2. handles | 3. , our framework for | 4. from and for the commercial world, including | 5. Rare Cases\n",
      "6. Spatial Databases | 7. Creating | 8. Documents and Images. | 9. graphics processing units | 10. guide to the\n",
      "11. Use cases | 12. and are represented by a | 13. Declarative | 14. Ada Tasking | 15. Syntactical\n",
      "16. are extracted from those regions. We propose the | 17. constructs as | 18. . While | 19. , however, cannot be efficiently executed on uniprocessor hardware without transformation. Some relevant kinds of transformation are mentioned, and the | 20. Abstract Interpretation\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 6\n",
    "cluster_dict = perform_clustering_kmeans(model, num_clusters)\n",
    "print_cluster_samples(cluster_dict, num_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use GaussianMixture to perform clustering and randomly print 20 phrases in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Analysis Results:\n",
      "==================================================\n",
      "\n",
      "Cluster 1 (Total words: 6483):\n",
      "--------------------------------------------------\n",
      "1. retrieving information | 2. Contours and Surfaces. | 3. data acquisition | 4. process is to perform a | 5. approach is an appealing way to depict people in a\n",
      "6. für Lernbehinderte und Hochbegabte. | 7. partition | 8. of 2D images and | 9. Protocols, | 10. Object Identity\n",
      "11. Book | 12. . Also, it can be composed of combinations of point and | 13. performance study | 14. user input | 15. mehrschichtige\n",
      "16. presents a system for | 17. -use 3D | 18. , Beschreibungs- und Ausführungsmodell | 19. Directory | 20. chosen for our\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2 (Total words: 3576):\n",
      "--------------------------------------------------\n",
      "1. Relational Database | 2. also grateful for the detailed comments provided by the other | 3. Formalization | 4. Computer Graphics | 5. The Baseline\n",
      "6. Topography | 7. degree | 8. procedural | 9. Cosine | 10. Communications.\n",
      "11. and look at several properties of | 12. intensities of the images | 13. methods for | 14. quality assessment | 15. Auswertung\n",
      "16. existing schemes | 17. grading | 18. RACE | 19. relational model | 20. geometrically accurate\n",
      "\n",
      "\n",
      "\n",
      "Cluster 3 (Total words: 123):\n",
      "--------------------------------------------------\n",
      "1. chapter | 2. language | 3. knowledge | 4. color | 5. , a\n",
      "6. software | 7. are | 8. / | 9. real-time | 10. Systems.\n",
      "11. Model | 12. This | 13. and the | 14. ) | 15. major\n",
      "16. multidimensional | 17. second | 18. global | 19. robust | 20. A\n",
      "\n",
      "\n",
      "\n",
      "Cluster 4 (Total words: 6649):\n",
      "--------------------------------------------------\n",
      "1. include those concerned with the | 2. shutter, it is possible to acquire \"slices\" of the scene at specific known distances. We show that even with large | 3. deserved more attention, because it | 4. mobile robot | 5. Problem Revisited\n",
      "6. -sets that is utilizing | 7. process is used to print the | 8. -based and | 9. , to derive the underlying | 10. who have not studied\n",
      "11. to the 3D | 12. Numerical Integration | 13. command, these concepts are | 14. Business Model | 15. Modal Logics\n",
      "16. . We analyze the | 17. hearing | 18. Software-Architekturen | 19. informed | 20. scanned data\n",
      "\n",
      "\n",
      "\n",
      "Cluster 5 (Total words: 24):\n",
      "--------------------------------------------------\n",
      "1. data | 2. range | 3. in | 4. information | 5. book\n",
      "6. . The | 7. ( | 8. . | 9. : | 10. results\n",
      "11. for | 12. database | 13. of the | 14. and | 15. of\n",
      "16. - | 17. to | 18. algorithm | 19. the | 20. paper\n",
      "\n",
      "\n",
      "\n",
      "Cluster 6 (Total words: 6670):\n",
      "--------------------------------------------------\n",
      "1. . It illustrates the | 2. only. For | 3. Cross-Cultural | 4. Realizing | 5. hardware and\n",
      "6. und andere | 7. to estimate | 8. shape change | 9. primitive types | 10. and the computation time. We\n",
      "11. application requirements | 12. Accuracy of 3D | 13. Nearest Neighbor | 14. Data mining | 15. features from\n",
      "16. Heterogeneous | 17. Alignments. | 18. measurements. | 19. Missing Data | 20. Computer Scientists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 6\n",
    "cluster_dict = perform_clustering_gmm(model, num_clusters)\n",
    "print_cluster_samples(cluster_dict, num_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that for Clusters 3 and 5, the phrases in these clusters are nearly identical across the two different clustering methods. Many phrases in these clusters consist of only a single word or are associated with prepositions or punctuation. For the other four clusters, there are some differences between KMeans and GMM; however, each KMeans cluster could correspond to a specific cluster in GMM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
