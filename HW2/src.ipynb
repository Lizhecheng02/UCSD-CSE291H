{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import defaultdict\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_segment(text):\n",
    "    return text.strip().replace(\" \", \"_\") if text.strip() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_segmented_corpus(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    processed_sentences = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        tokens = []\n",
    "        current_pos = 0\n",
    "        line_length = len(line)\n",
    "\n",
    "        while current_pos < line_length:\n",
    "            next_phrase_start = line.find(\"<phrase_Q=\", current_pos)\n",
    "\n",
    "            if next_phrase_start == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            if next_phrase_start > current_pos:\n",
    "                pre_phrase_text = line[current_pos:next_phrase_start]\n",
    "                pre_phrase_token = process_text_segment(pre_phrase_text)\n",
    "                if pre_phrase_token:\n",
    "                    tokens.append(pre_phrase_token)\n",
    "\n",
    "            phrase_header_end = line.find(\">\", next_phrase_start)\n",
    "            if phrase_header_end == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            phrase_end = line.find(\"</phrase>\", phrase_header_end)\n",
    "            if phrase_end == -1:\n",
    "                remaining_token = process_text_segment(line[current_pos:])\n",
    "                if remaining_token:\n",
    "                    tokens.append(remaining_token)\n",
    "                break\n",
    "\n",
    "            phrase = line[phrase_header_end + 1:phrase_end]\n",
    "            if phrase:\n",
    "                phrase_token = phrase.replace(\" \", \"_\")\n",
    "                tokens.append(phrase_token)\n",
    "\n",
    "            current_pos = phrase_end + len(\"</phrase>\")\n",
    "\n",
    "        if tokens:\n",
    "            processed_sentences.append(tokens)\n",
    "\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentences, vector_size=100, window=5, min_count=1):\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 5000\n",
      "Sentence 1: ['OQL', '[', 'C++', ']:', 'Extending', 'C++', 'with_an_Object_Query_Capability.']\n",
      "Sentence 2: ['Transaction_Management', 'in', 'Multidatabase_Systems', '.']\n",
      "Sentence 3: ['Overview', 'of_the_ADDS_System.']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./AutoPhrase/models/DBLP/segmentation.txt\"\n",
    "\n",
    "sentences = parse_segmented_corpus(file_path)\n",
    "print(\"Total number of sentences:\", len(sentences))\n",
    "\n",
    "for i, sentence in enumerate(sentences[:3]):\n",
    "    print(f\"Sentence {i + 1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 23525\n",
      "Example vectors:\n",
      "\n",
      "Token: OQL\n",
      "Vector shape: (100,)\n",
      "First 5 dimensions: [-0.00323956  0.00342402 -0.00773631  0.00460851  0.00436019]\n",
      "\n",
      "Token: [\n",
      "Vector shape: (100,)\n",
      "First 5 dimensions: [ 0.00426744 -0.00132468  0.00022595 -0.00935194  0.00950344]\n"
     ]
    }
   ],
   "source": [
    "model = train_word2vec(sentences)\n",
    "print(f\"Total vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "\n",
    "print(\"Example vectors:\")\n",
    "for sentence in sentences[:1]:\n",
    "    for token in sentence[:2]:\n",
    "        print(f\"\\nToken: {token}\")\n",
    "        print(f\"Vector shape: {model.wv[token].shape}\")\n",
    "        print(f\"First 5 dimensions: {model.wv[token][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_kmeans(model, num_clusters=5):    \n",
    "    vectors = []\n",
    "    words = []\n",
    "    for word in model.wv.index_to_key:\n",
    "        vectors.append(model.wv[word])\n",
    "        words.append(word)\n",
    "    \n",
    "    X = np.array(vectors)\n",
    "    print(X.shape)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    cluster_dict = defaultdict(list)\n",
    "    for word, cluster in zip(words, clusters):\n",
    "        cluster_dict[cluster].append(word)\n",
    "    \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_gmm(model, num_clusters=6):\n",
    "    vectors = []\n",
    "    words = []\n",
    "    for word in model.wv.index_to_key:\n",
    "        vectors.append(model.wv[word])\n",
    "        words.append(word)\n",
    "    \n",
    "    X = np.array(vectors)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=num_clusters, random_state=42)\n",
    "    clusters = gmm.fit_predict(X)\n",
    "\n",
    "    cluster_dict = defaultdict(list)\n",
    "    for word, cluster in zip(words, clusters):\n",
    "        cluster_dict[cluster].append(word)\n",
    "    \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_samples(cluster_dict, num_samples=20):\n",
    "    print(\"Cluster Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for cluster_id in sorted(cluster_dict.keys()):\n",
    "        words = cluster_dict[cluster_id]\n",
    "        samples = words if len(words) <= num_samples else random.sample(words, num_samples)\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id + 1} (Total words: {len(words)}):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, word in enumerate(samples, 1):\n",
    "            print(f\"{i}. {word.replace('_', ' ')}\", end=\"\\n\" if i % 5 == 0 else \" | \")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23525, 100)\n",
      "Cluster Analysis Results:\n",
      "==================================================\n",
      "\n",
      "Cluster 1 (Total words: 6388):\n",
      "--------------------------------------------------\n",
      "1. Inverse | 2. FM8501: A Verified | 3. triangle mesh | 4. Realisierung von | 5. distributed database\n",
      "6. decided to start | 7. POCO | 8. type inheritance | 9. Reduction and | 10. Solar\n",
      "11. subdivision technique. The | 12. are mostly concerned with | 13. Planen | 14. , or transmit classified | 15. shortest paths\n",
      "16. Automatic Text | 17. under severe changes of | 18. systems in | 19. of DebitCredit and the | 20. lightweight\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2 (Total words: 4210):\n",
      "--------------------------------------------------\n",
      "1. the seats'''' means the seats of Nadia'' | 2. Constraint Services: | 3. on the authors' | 4. change | 5. push\n",
      "6. MRF | 7. Inconsistent | 8. divided into | 9. curve in 3D space. | 10. career\n",
      "11. On the Convergence of Analysis and | 12. . The 3D | 13. medial axis | 14. Systeme | 15. distance transform\n",
      "16. . The specification | 17. C | 18. programmer | 19. Topology | 20. methodology for analysis and\n",
      "\n",
      "\n",
      "\n",
      "Cluster 3 (Total words: 135):\n",
      "--------------------------------------------------\n",
      "1. multi | 2. resolution | 3. algorithms | 4. 3D | 5. global\n",
      "6. software | 7. correspondence | 8. Advanced | 9. area | 10. databases\n",
      "11. dynamic | 12. visual | 13. Auflage | 14. UML | 15. A\n",
      "16. , the | 17. geometry | 18. Experimental | 19. stereo | 20. state\n",
      "\n",
      "\n",
      "\n",
      "Cluster 4 (Total words: 6169):\n",
      "--------------------------------------------------\n",
      "1. Discrete Geometry | 2. XHTML | 3. Embedded Devices | 4. our scene with a | 5. the components, and it occurred to\n",
      "6. Urheberrecht und | 7. . By using the | 8. the variations of the | 9. Third | 10. arc\n",
      "11. Queueing Models | 12. und Diagnose | 13. and similarities among these systems are analyzed in relation to the definition, | 14. and Its Applications to Valid-time | 15. between contiguous codewords increases. As a consequence of that, code loss circumstances can be\n",
      "16. that avoids the above problems by | 17. anaphoric | 18. simulator | 19. , based on the | 20. Visualisation\n",
      "\n",
      "\n",
      "\n",
      "Cluster 5 (Total words: 25):\n",
      "--------------------------------------------------\n",
      "1. the | 2. results | 3. information | 4. . | 5. of\n",
      "6. paper | 7. ( | 8. : | 9. in | 10. . The\n",
      "11. algorithm | 12. book | 13. of the | 14. data | 15. for\n",
      "16. - | 17. D | 18. and | 19. database | 20. camera\n",
      "\n",
      "\n",
      "\n",
      "Cluster 6 (Total words: 6598):\n",
      "--------------------------------------------------\n",
      "1. Projections on a | 2. people in the same way? The innocence of | 3. . We describe an | 4. concatenation | 5. Objects: Issues, Solutions, and Challenges.\n",
      "6. Systematische | 7. . But using | 8. Graph Isomorphism | 9. must be extracted from the scene in | 10. assertional reasoning\n",
      "11. ALGOL | 12. computational complexity | 13. Physical Database Design | 14. measurement | 15. developed several\n",
      "16. , we propose a novel method for | 17. Based on Time Sequences. | 18. of forestalling common flaws in | 19. telephone | 20. Collaborative Problem Solving\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 6\n",
    "cluster_dict = perform_clustering_kmeans(model, num_clusters)\n",
    "print_cluster_samples(cluster_dict, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Analysis Results:\n",
      "==================================================\n",
      "\n",
      "Cluster 1 (Total words: 6369):\n",
      "--------------------------------------------------\n",
      "1. didn't need them. What | 2. Outlier | 3. B+-Tree | 4. of Reasoning with | 5. and is meant to be read from beginning to end. It explains how to specify the class of properties known as\n",
      "6. also refers | 7. temporally | 8. left unsampled. Both the initial set of | 9. of Proofsystems for | 10. to a limited number of people is now instantly retrievable anywhere in the world by anyone with a computer and an\n",
      "11. images. This | 12. Elimination | 13. Seabottom Surveys. | 14. body | 15. sixth\n",
      "16. set by certain simple operations. Interconnections between simplicity of computations and | 17. and to compensate for the | 18. differ significantly | 19. Phase Unwrapping | 20. fields and\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2 (Total words: 3332):\n",
      "--------------------------------------------------\n",
      "1. Natural images | 2. approach produces a consistent | 3. Structural Mechanics | 4. on cost and benefit. The segment cleaner decides which segments to clean based on a | 5. sampling\n",
      "6. target | 7. Datenbanken | 8. , implementing, and | 9. . The system is more likely to help implement and maintain | 10. Course\n",
      "11. . Our method builds on | 12. trees | 13. to simplify | 14. views at certain points (key-positions) along that path (one view per position). An | 15. measurement matrix\n",
      "16. local network | 17. Frameworks and | 18. VSM | 19. diffuse reflectance | 20. Data Access\n",
      "\n",
      "\n",
      "\n",
      "Cluster 3 (Total words: 135):\n",
      "--------------------------------------------------\n",
      "1. Language | 2. efficient | 3. area | 4. Model | 5. in the\n",
      "6. basic | 7. or | 8. modeling | 9. extended | 10. on the\n",
      "11. Finally | 12. data model | 13. research | 14. I | 15. active\n",
      "16. general | 17. Using | 18. high | 19. object-oriented | 20. Prolog\n",
      "\n",
      "\n",
      "\n",
      "Cluster 4 (Total words: 6742):\n",
      "--------------------------------------------------\n",
      "1. Modern Information Retrieval | 2. Petrinetze, | 3. querying databases | 4. Object-Role Modeling | 5. ) has become widely used for\n",
      "6. to compute with the | 7. to articulated or more generally | 8. . We spent as little time as possible on the | 9. equally important | 10. Konstruktion\n",
      "11. the approach extensively using real examples of hard | 12. Scanning for Humans and Other | 13. The Analysis of a | 14. raycasting | 15. straight and curved\n",
      "16. Moire | 17. of Inconsistent and Default | 18. Binary | 19. bildhaften | 20. error over\n",
      "\n",
      "\n",
      "\n",
      "Cluster 5 (Total words: 25):\n",
      "--------------------------------------------------\n",
      "1. for | 2. paper | 3. in | 4. with | 5. information\n",
      "6. and | 7. of the | 8. model | 9. book | 10. D\n",
      "11. , | 12. camera | 13. to | 14. data | 15. :\n",
      "16. . | 17. range | 18. database | 19. ( | 20. . The\n",
      "\n",
      "\n",
      "\n",
      "Cluster 6 (Total words: 6922):\n",
      "--------------------------------------------------\n",
      "1. dedicated to a particular domain. Our examples concern | 2. , we present a clientserver system allowing | 3. An Exploratory | 4. Molecular Biological | 5. for the Future of\n",
      "6. Patterns to Object and | 7. built upon | 8. Compilers | 9. and Guiding | 10. Principles to Guide the\n",
      "11. Java 2 Micro Edition | 12. electrical | 13. Intranet | 14. -80 system. It is | 15. managed by PICDMS or other\n",
      "16. society's | 17. can be used in applications such as | 18. Architectures for Different | 19. On the Potential of | 20. PostScript\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 6\n",
    "cluster_dict = perform_clustering_gmm(model, num_clusters)\n",
    "print_cluster_samples(cluster_dict, num_samples=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
